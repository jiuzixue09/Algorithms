## 分而治之

### 方法介绍
对于海量数据而言，由于无法一次性装进内存处理，导致我们不得不把海量的数据通过hash
映射分割成相应的小块数据，然后再针对各个小快数据通过hash_map进行统计或其它操作。

### 问题实例


**1. 海量日志数据，提取某日访问百度次数最多的那个IP**

**分析**：由于IP数量巨大，不能一次性装进内存处理，这时候可以把大文件转化成（取模映射）小文件，从而大而化小，逐个处理。

**解法**：3个步骤


1. hash映射

    - 先将这一天访问百度日志的所有IP提取出来，逐个写入到一个大文件中，使用映射方法,比如 %1000, 把整个大文件映射为1000个小文件。
1. hash_map统计

    - 使用hash_map<ip, value>分别对1000个小文件的IP使用频率统计，找出每个小文件中出现频率最高的IP。

1. 堆/快速排序

    - 统计1000个频率最大的IP后，按频率大小排序，找出频率最大的IP

**注**：Hash取模是一种等价映射，不会把同一个元素分散到不同小文件中，即同一个IP只可能落在同一个文件中。

**2. 寻找热门查询，300万个查询字符串中统计最热门的10个查询**

**原题**：搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为1-255字节。假设目前有一千万个记录，请你统计最热门的10个查询串，要求使用的内存不能超过1G。
